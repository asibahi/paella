# Writing a C Compiler - Chapter 2 - in Zig

So, with the [first chapter](c1.md) of [Writing a C Compiler](https://norasandler.com/2022/03/29/Write-a-C-Compiler-the-Book.html) behind us, it is time to start on the swcond chapter: Unary Operators. But first, a word from `build.zig` land.

## Running the tests with `zig build test`

The tests for ths Book live in [their own repositry](https://github.com/nlsandler/writing-a-c-compiler-tests), which I have dutifully cloned and run tests in.

The process I followed to run the tests is fairly manual:

1. navigate to the test directory.
2. run the command `arch -x86_64 zsh` (which I have cleverly aliased to `x86`). This runs a x86 version of `zsh`.
3. run the tests.

When I was doing [my Rust implementation](https://github.com/asibahi/trjm), I kept a terminal accessible with a global hotkey and I kept the inner shell running at all times.

With `build.zig`, I looked for a way to do that automatically. The `Build` object, which is central to `build.zig` has a nice `addSystemCommand` method, which I could use to run my stuff in. Now remained the problem of how to run the tests in the x86 shell.

My first thought was to have the command as `arch -x86_64 test_compiler ..etc` (`test_compiler` being the test script provided). While it worked, as in the tests for chapter 1 all ran fine, upon further reading I realized that the `arch` actually chooses which binary to run from a macOS Universal Binary. It has no effect on python scripts, and the simple tests on chapter 1 worked fine either way. So I asked around, and some kind soul pointed me to the right thing to search for: Subshells.

See, this might seem obvious if you've lived in Unix land all your life, but shells are applications too. You can pass command line arguments to them!! So, `bash -c "echo foo"`, runs `bash`, runs the command `echo foo` in `bash`, then exits. Pefect.

Putting these two together, this was the added bit to `build.zig`:

```zig
const test_step = b.step("test", "Run tests");

// subshells. how do they work.
const inner_command = try std.mem.join(b.allocator, " ", &.{
    "../writing-a-c-compiler-tests/test_compiler",
    b.pathJoin(&.{ b.exe_dir, "paella" }),
    try std.mem.join(
        b.allocator,
        " ",
        b.args orelse &.{""},
    ),
});

// does this work like i think it does?
const test_command = b.addSystemCommand(
    &.{ "arch", "-x86_64", "zsh", "-c", inner_command },
);

test_command.step.dependOn(b.getInstallStep());
test_step.dependOn(&test_command.step);
```

I am unreasonably happy with this. Using regular Zig standard library tools like `std.mem.join` allowed to join together the commands I am passing to the `zsh` subshell, and voila!!

Now the tests (say for chapter 1) can be ran "simply" with `zig build test -- --chapter 1`. Great success!

## Lexer

There are three tokens to add in this chapter: `~`, `-`, and `--`. The decrement operator is only being tokenized here to reject illegal C syntax like `return --2`, but otherwise will not be implemented.

[^main]: I refactored the code a bit after chapter 1 and added proper support for the `idenitfier` token. It is a trivial change so no point in detailing it further, except that the new `expect` is cooler than before.

So our `Tag` enum grows[^main] to have these three tokens: `tilde, hyphen`. As I am not planning to implement the book's extra credit this time around, I will just have the double hyphen lex into the `invalid` token. This would reject lexing valid code like `x--`, but I do not think this is going to be present into the test cases. We will see.

The `State` enum is a bit more interesting. It cannot accept two consecutive hyphens, so seeing a hyphen would put it into the `hyphen` state, where if it sees another hyphen, it puts out `Tag.invalid`, or `Tag.hyphen` otherwise.

So, if the state is at `.start`, and the lexer encounters a `'-'` character, it just switches gear to the `.hyphen` state. Labelled `switch` in action. Then in there, triage happens. The code is honestly copied, again, from the Zig compiler, with adjustments.

```zig
state: switch (State.start) {
    .start => switch (self.buffer[self.index]) {
        '~' => {
            result.tag = .tilde;
            self.index += 1;
        },
        '-' => continue :state .hyphen,
        // etc
    .hyphen => {
        self.index += 1;
        switch (self.buffer[self.index]) {
            '-' => result.tag = .invalid,
            else => result.tag = .hyphen,
        }
    },
    // etc
}
```

This should be it. Time to put the new testing command into action.

```
Ran 43 tests in 5.432s

OK
```

Both of them work. Time for AST and parsing.

## Parsing

Changes to the AST are fairly minimal. There are two unary operations: negation, and complement. Instead of adding a spearate `UnaryOp` enum as the Book suggests, I am going to embed them right into `Expr` as spearate tags, updating the pretty printer while I am at it.[^json]

[^json]: There is an argument to be made for replacing my custom pretty printer with standard library's `json` serilaizr. The result is more verbose and has more vertical space and redundant info, but on the plus side, it requires zero maintenance.

```zig
pub const Expr = union(enum) {
    constant: u64,
    unop_negate: *Expr,
    unop_complement: *Expr,

    pub fn format(
        self: @This(),
        comptime _: []const u8,
        _: std.fmt.FormatOptions,
        writer: anytype,
    ) !void {
        switch (self) {
            .constant => |c| try writer.print("{d}", .{c}),
            .unop_negate => |e| try writer.print(" -{}", .{e}),
            .unop_complement => |e| try writer.print(" ~{}", .{e}),
        }
    }
};
```

Updating the parsing functions is a bit more involved, however. Now is the start of PEMDAS, and time to start our recursive descemt into madness.[^madness] This is pretty much the same function provided in the book, except that the operation is chosen immediately.

[^madness]: Sounds good for a book title or something, doesn't it?

```zig
fn parse_expr(
    alloc: std.mem.Allocator,
    tokens: *lexer.Tokenizer,
) !*ast.Expr {
    const current = tokens.next() orelse
        return error.ExpectExpr;

    switch (current.tag) {
        .number_literal => {
            const lit = tokens.buffer[current.loc.start..current.loc.end];
            const res = try std.fmt.parseInt(u64, lit, 10);

            //         vvv this helper function just allocates and sets
            return try create(ast.Expr, alloc, .{ .constant = res });
        },
        .hyphen => {
            const inner_exp = try parse_expr(alloc, tokens);
            return try create(ast.Expr, alloc, .{ .unop_negate = inner_exp });
        },
        .tilde => {
            const inner_exp = try parse_expr(alloc, tokens);
            return try create(ast.Expr, alloc, .{ .unop_complement = inner_exp });
        },
        .l_paren => {
            const inner_exp = try parse_expr(alloc, tokens);
            try expect(.r_paren, tokens);

            return inner_exp;
        },
        else => return error.ExpectExpr,
    }
}
```

Now the parsing is done, but to test a small annoyance needs to be fixed. In `asm_gen.zig`, there is a switch over expressions, and it needs to handle the new modes, eben tho it is not being called right now. This tedium and the need to repeat it for every step is partially why I stopped the Rust implementation. Adding a `else => @panic("unimplemented"),` to that `switch` smoothes things over for now.

```
Ran 43 tests in 6.363s

OK
```

Yuppee.

Last but not least, even though it is not needed for the first chapters, as the only idenitfier is `main`, I figured I'd try some `comptime` magic to improve my expectation experiance. I changed `expect` a little bit so it returns a string when I am asking for an identifier, but `void` otherwise.[^matklad]

```zig
fn expect(
    comptime expected: lexer.Token.Tag,
    tokens: *lexer.Tokenizer,
) !if (expected == .identifier)
    []const u8
else
    void {
    if (tokens.next()) |actual| {
        if (actual.tag != expected)
            return error.SyntaxError;
        if (expected == .identifier)
            return tokens.buffer[actual.loc.start..actual.loc.end];
    } else return error.SyntaxError;
}
```

[^matklad]: Inspired by [matklad](https://matklad.github.io/2025/04/21/fun-zig-program.html).

## Intermediate Representation

The Book uses its own version of Intermediate Representation, called `TACKY`. It is a variation of a popular IR strategy called _three-address code_, heance the name.

`TACKY` uses its own AST, and it would slot between the program's AST and the assembly's AST. This is the first pass. So many strings.

```zig
const std = @import("std");

pub const Prgm = struct {
    func_def: *FuncDef,
};

pub const FuncDef = struct {
    name: []const u8,
    // for similar reasons to its use in assembly AST
    instrs: std.ArrayListUnmanaged(Inst),
};

pub const Inst = union(enum) {
    ret: Value,

    // decided to splat the operator here, like discussed in chapter 1.
    unop_negate: Unary,
    unop_complement: Unary,

    // to avoid excessive typing
    pub const Unary = struct {
        src: Value,
        dst: Value,

        pub fn init(src: Value, dst: Value) @This() {
            return .{ .src = src, .dst = dst };
        }
    };
};

pub const Value = union(enum) {
    constant: u64,
    variable: []const u8,
};
```

The goal of this IR is to disentangle nested expressions by separating them into what is, at least in IR form, separate operations. This means that for an expression like `1 + 2 * 3`[^binary], it is transformed into the following representation:

[^binary]: Binary operations are in the next chapter.

```python
tmp0 = 2 * 3
tmp1 = 1 + tmp0
return tmp1
```

The Book provided `expr_emit_ir`, with my splatted data structures, looks like the following:

```zig
fn expr_emit_ir(
    alloc: std.mem.Allocator,
    expr: *ast.Expr,
    instrs: *std.ArrayListUnmanaged(ir.Instr),
) !ir.Value {
    switch (expr.*) {
        .constant => |c| return .{ .constant = c },
        .unop_negate => |e| {
            const src = try expr_emit_ir(alloc, expr, instrs);
            const dst_name = try make_temporary(alloc, "neg");
            const dst: ir.Value = .{ .variable = dst_name };
            try instrs.append(alloc, .{ .unop_negate = .init(src, dst) });
            return dst;
        },
        .unop_complement => |e| {
            const src = try expr_emit_ir(alloc, expr, instrs);
            const dst_name = try make_temporary(alloc, "cml");
            const dst: ir.Value = .{ .variable = dst_name };
            try instrs.append(alloc, .{ .unop_complement = .init(src, dst) });
            return dst;
        },
    }
}
```

A bit repititive, isn't it? So I put together the repeated logic into its own function, `unary_helper`:

```zig
fn unary_helper(
    alloc: std.mem.Allocator,
    expr: *ast.Expr,
    instrs: *std.ArrayListUnmanaged(ir.Instr),
    comptime prefix: []const u8,
) !struct { ir.Value, ir.Value } {
    const src = try expr_emit_ir(alloc, expr, instrs);
    const dst_name = try make_temporary(alloc, prefix);
    const dst: ir.Value = .{ .variable = dst_name };

    return .{ src, dst };
}
```

Nothing fancy. `make_temporary` over here is just a function with an ever increasing static variable for creating always different variable names. However, I hit here the weirdest zig compile error I have seen, yet. `error: unable to resolve inferred error set`. What gives?

Turns out when two recursive functions call each other, both with an inferred error type, Zig throws in the towel. Swift would've never given up and compiled, eventually;

Apparently I had to specify the error type myself.

```zig
const helper_error = error{
    MakeTemporary, // return error value of `make_temporary`
    UnaryHelper,
};
```

... and changed `unary_helper` to this:

```zig
fn unary_helper(
    alloc: std.mem.Allocator,
    expr: *ast.Expr,
    instrs: *std.ArrayListUnmanaged(ir.Instr),
    comptime prefix: []const u8,
) helper_error!struct { ir.Value, ir.Value } {
    const src = expr_emit_ir(alloc, expr, instrs) catch
        return error.UnaryHelper;
    const dst_name = try make_temporary(alloc, prefix);
    const dst: ir.Value = .{ .variable = dst_name };

    return .{ src, dst };
}
```

And now it compiles. The rest of `ir_gen.zig`, is rahter a straightforward one to one between the two trees, and rather boring to implement. Instead, I will show you `make_temporary`. Zig has a weird way to define local static variables.

```zig
fn make_temporary(
    alloc: std.mem.Allocator,
    comptime prefix: []const u8,
) helper_error![]const u8 {

    // zig static variables
    const coun = struct {
        var ter: usize = 0;
    };

    const name = std.fmt.allocPrint(
        alloc,
        if (prefix.len == 0) "temp" else prefix ++ ".{}",
        .{coun.ter},
    ) catch return error.MakeTemporary;
    coun.ter += 1;

    return name;
}
```

First of all, non-Atomic is fine. All this is single threaded anyway. Second of all, any _variable_ directly declared in a `struct` scope is static. So if you need a local static variable you define a `struct` with a .. variable. This is exactly the same semantics as declaring a global variable (or at file scope) except the globe here is a lot smaller.[^struct] Mind bending a bit, but makes sense.

[^struct]: Helps to keep in mind that every file in Zig is a struct, too.

To save on pretty printing this time, I figured to try `std.json.stringify` to get the IR output. This is the C file I am working on. So small and non-threatening.

```c
int main(void) {
    return ~-3;
}
```

And this is the IR output:

```json
{
	"func_def": {
		"name": "main",
		"instrs": {
			"items": [
				{
					"unop_negate": {
						"src": {
							"constant": 3
						},
						"dst": {
							"variable": "neg.0"
						}
					}
				},
				{
					"unop_complement": {
						"src": {
							"variable": "neg.0"
						},
						"dst": {
							"variable": "cml.1"
						}
					}
				},
				{
					"ret": {
						"variable": "cml.1"
					}
				}
			],
			"capacity": 5
		}
	}
}
```

Very longwinded, with some useless info. Why do I need to know the `ArrayListUnmanaged` capacity? Time to work on pretty printing and allocation bookkeeping. Not very interesting, to be honest, so I will skip writing about it.

Except for string interning. that is interesting.

## String Interning

String interning is having a single storage for strings that are used throught the program. Keeping the source around is not enough, because of the new strings for the IR's temporary variables. Giving ownership of the strings to `ir.Value` is _fine_, except it is really easy to double free the strings when cleaning up. In fact, I did that when I was trying to not implement the string interner, and Zig's `DebugAllocator` dutifully caught it.

The basic idea behind the interner is to have all strings owned by one repositry that you can free at once at the end. One can use an arena, but the interner's structure is more efficient because it also avoids duplication of strings. It can be backed by a hashmap, so it can also attach information to each string. All in all it is a useful memory and resource management technique.

The friendly people at the Zig discord, especially [InKryption](https://github.com/InKryption) not only helped me out understand the data idea, bu actually provided me with a complete implementation.

It makes use of Zig's, hoenstly weird, sentinel-terminated arrays: a generalization of C's null-terminated strings.

The idea is as follows: a regular slice (say the `[]comst u8` we use and love) stores its length within it. It knows what length it is at all times. So if you read from it or iterate on it, it stops dutifully at the length it knows. A null-terminated string, however, would be` [:0]const u8`. When you read from it or iterate on it, it keeps going until it hits the `0`. (Obviously you can use any sentinel value but the `0` is very convenient for strings.)

So the String Interner keeps as its backing data a regular `ArrayListUnmanaged(u8)` that I have been using before. Say it is called `bytes`.) But for any new string in the application, it appends it to the ArrayList then adds `0` byte at the end. And I keep its _starting_ index around. When I want to use it, I take slice like so `bytes[starting_idx..:0]` and voila .. I have a slice of my string.

The remaining pieces of the puzzle is two items in the Zig standard library: `StringIndexContext` and `StringIndexAdapter`. They're specifically designed for this specific use case. Who knew?

A `Context` in Zig's hash maps is a simple enough idea. It just defines the `eql` and `hash` functions said hash map will use, with an optional backing data structe. An `Adapter` is used to override these two functions for speciifc operations. The `StringIndex` duo have slightly different implementation of the `eql` function, and if you're curious you can check them out in the standard library's code.[^adapters]

[^adapters]: A useful read, which was educational for me on Contexts and Adaptors (in current Zig version, at least), are the Zig issues [#21917](https://github.com/ziglang/zig/issues/21917) and [#23872](https://github.com/ziglang/zig/issues/23872), especially [mulgg](https://github.com/mlugg)'s comments. On the zig discord, mlugg gave me this useful summary: "they're quite similar. a context is essentially an adapter but for the 'real' key type; a context tells you how to hash/eql the actual `comptime K: T`, whereas an adapter allows you to insert _different_ key types as long as you can implement _compatible_ hash/eql on them".

Without further ado, here is the full `StringInterner` provided thankfully by InKryption. I honestly had to read multiple times to wrap my head around what's going on, so I commented some parts of it.

```zig
const std = @import("std");
const StringInterner = struct {
    // state
    bytes: std.ArrayListUnmanaged(u8),
    map: std.HashMapUnmanaged(
        Id,
        void,
        std.hash_map.StringIndexContext,
        std.hash_map.default_max_load_percentage,
    ),

    // management
    pub const init: StringInterner = .{
        .bytes = .empty,
        .map = .empty,
    };

    pub fn deinit(self: *StringInterner, allocator: std.mem.Allocator) void {
        self.bytes.deinit(allocator);
        self.map.deinit(allocator);
    }

    // type alias. store this in stuff.
    pub const Idx = u32;

    // checks if a string is in or not. returns ID
    pub fn getIdx(
        self: *const StringInterner,
        string: []const u8,
    ) ?Idx {
        return self.map.getKeyAdapted(string, self.hmAdapter());
    }

    // get string from id
    pub fn getString(
        self: *const StringInterner,
        idx: Idx,
    ) ?[:0]const u8 {
        if (!self.map.containsContext(idx, self.hmCtx())) return null;

        // cast is necessary for type inference reasons.
        const slice_sentinel: [:0]const u8 = @ptrCast(self.bytes.items[idx..]);
        return std.mem.sliceTo(slice_sentinel, 0);
    }

    // the insert function. returns aither an existing idx or a new idx.
    pub fn getOrPut(
        self: *StringInterner,
        allocator: std.mem.Allocator,
        string: []const u8,
    ) std.mem.Allocator.Error!Idx {
        // reserves capacity in both the backing array and the map
        try self.bytes.ensureUnusedCapacity(allocator, string.len + 1);
        try self.map.ensureUnusedCapacityContext(allocator, 1, self.hmCtx());

        // getOrPut returns a reference to the key and the value. If it existed, they're
        // valid pointers. If not, their contents are undefined waiting for you to fill them.
        const gop = self.map.getOrPutAssumeCapacityAdapted(string, self.hmAdapter());

        gop.value_ptr.* = {}; // this is void, but it can be anything!! note void is zero-width
        if (gop.found_existing) return gop.key_ptr.*;

        // unlikelt to hit that. the source files are tiny.
        if (self.bytes.items.len > std.math.maxInt(Idx)) return error.OutOfMemory;
        const new_idx: Idx = @intCast(self.bytes.items.len);

        // indertion happens here
        self.bytes.appendSliceAssumeCapacity(string);

        // don't forget to append the null byte !!
        self.bytes.appendAssumeCapacity(0);

        // update the map through the pointer.
        gop.key_ptr.* = new_idx;
        return new_idx;
    }

    // helper functions to avoid a self referential struct
    fn hmCtx(self: *const StringInterner) std.hash_map.StringIndexContext {
        return .{ .bytes = &self.bytes };
    }
    fn hmAdapter(self: *const StringInterner) std.hash_map.StringIndexAdapter {
        return .{ .bytes = &self.bytes };
    }
};
```

I will currently use this structure as is, only changing the names to be `snake_case` as I cannot read `camelCase`. Later on I can use it to attach data to the strings. Fairly useful.

My first go at this was to replace every string with `StringInterner.Idx`. Just make sure to pass a pointer to the interner when constructing these structures.

```zig
pub const Prgm = struct {
    func_def: *FuncDef,
};

pub const FuncDef = struct {
    name: utils.StringInterner.Idx,
    instrs: std.ArrayListUnmanaged(Instr),
};

// and so on.
```

While this works and compiels fine, it turned out to be problematic when pretty printing. `Idx` is just a `u32` and stores no reference to the interner. And the `format` functions do not have an allocator or a user-space reference to use. So instead, I am changing the type to `[:0]const u8`, then storing slices from the interner rather than indices.

Then adjust our `ir_gen` functions to pass a pointer to the interner through all of them. That or creating a makeshift `Ctx` type. Time will tell. For an example, now, this the `unary_helper` mentioned earlier and the new `make_temporary`, with the new structure threaded in.

```zig

fn unary_helper(
    alloc: std.mem.Allocator,
    interner: *utils.StringInterner,
    expr: *ast.Expr,
    instrs: *std.ArrayListUnmanaged(ir.Instr),
    comptime prefix: []const u8,
) helper_error!struct { ir.Value, ir.Value } {
    const src = expr_emit_ir(alloc, interner, expr, instrs) catch
        return error.UnaryHelper;
    const dst_name = try make_temporary(alloc, interner, prefix);
    const dst: ir.Value = .{ .variable = dst_name };

    return .{ src, dst };
}

fn make_temporary(
    alloc: std.mem.Allocator,
    interner: *utils.StringInterner,
    comptime prefix: []const u8,
) helper_error![:0]const u8 {
    const static = struct {
        var counter: usize = 0;
    };

    const name = std.fmt.allocPrint(
        alloc,
        if (prefix.len == 0) "temp" else prefix ++ ".{}",
        .{static.counter},
    ) catch return error.MakeTemporary;
    defer alloc.free(name);

    const name_idx = interner.get_or_put(alloc, name) catch
        return error.MakeTemporary;
    const name_res = interner.get_string(name_idx).?; // <- unwrap optional

    static.counter += 1;

    return name_res;
}
```

So this all compiles and runs and passes tests just fine. All is left is implementing the pretty printing myself, as the `json` module seemed to break for reasons I do not understand (related to the interner).[^json_bug] The technique to do so was discussed in the Chapter 1 article, so no point in repeating it here. It is time to move on to the next seciotn.

[^json_bug]: The crash disappeared when I removed the needless reference to the intenrer from the `ir.Prgm` struct, but I hit a new problem. The null terminated strings were printing as `"main\u0000"`. I am not sure if this is a standard library bug or a bug in my code. Definitely a case of debugging my language knowledge.
